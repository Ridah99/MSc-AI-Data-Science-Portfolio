{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXptpBMiLoK4"
      },
      "source": [
        "<h1>A Machine Learning Hybrid Approach for PCOS Detection Using Ovarian Ultrasound Images</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeX8wtuoLbmo"
      },
      "source": [
        "<h2> The cells below visualize the specifications of the virtual system used in this notebook</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opuCz8Ak1KEy"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Installing Kaggle<h3>"
      ],
      "metadata": {
        "id": "njF1JBqha0_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7WvCxcG8ChM"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Mount google drive to save the models and other variables in this notebook <h3>"
      ],
      "metadata": {
        "id": "bYJcgXHn576a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S81vefKZM1ro"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Several operations were carried out below to download the datast from Kaggle </h3>"
      ],
      "metadata": {
        "id": "R4_Cu_5D68mR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2gQpTFCM7zT"
      },
      "outputs": [],
      "source": [
        "# Making a directory in the colab session that would hold my kaggle API\n",
        "\n",
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dRBFuwiN2kk"
      },
      "outputs": [],
      "source": [
        "# Copying the kaggle API credentials file to the newly created kaggle directory\n",
        "\n",
        "!cp /content/drive/MyDrive/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsx7Oa0DOBRd"
      },
      "outputs": [],
      "source": [
        "# Using the kaggle API to access my kaggle account\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RYcUOXGTUBp"
      },
      "outputs": [],
      "source": [
        "# Dowloading the pcos-detection-using-ultrasound-images dataset into the collab session using my kaggle account\n",
        "\n",
        "! kaggle datasets download -d anaghachoudhari/pcos-detection-using-ultrasound-images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EzIn3P_OAvq"
      },
      "outputs": [],
      "source": [
        "# Unzipping the pcos-detection-using-ultrasound-images dataset because Kaggle datasets are often in zipped format\n",
        "\n",
        "! unzip /content/pcos-detection-using-ultrasound-images.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Importing the libraries used in this notebook </h3>"
      ],
      "metadata": {
        "id": "q_A_vezj5hia"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ufJda8dKdaa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuGCfL5YMFwK"
      },
      "source": [
        "<h2>Data Preprocessing</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-ZwPXSpQ_qM"
      },
      "source": [
        "<h4>Define the data directory</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY6QI3n6MMAk"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing the training dataset\n",
        "\n",
        "data_dir = '/content/data/train'\n",
        "\n",
        "# Define the directory containing the testing dataset\n",
        "\n",
        "test_dir = '/content/data/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acCkx5TuRSZw"
      },
      "source": [
        "<h4>Define the batch size and image size</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjPovFgTMQh3"
      },
      "outputs": [],
      "source": [
        "# Define the batch size for training\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Define the dimensions for the images\n",
        "\n",
        "img_height = 224\n",
        "\n",
        "img_width = 224"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stT2x2WBRrW4"
      },
      "source": [
        "<h4>Preprocess the train data into training and validation set using the image_dataset_from_directory method</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jitFcUW8MetX"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the train dataset\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "data_dir,\n",
        "validation_split = 0.2,\n",
        "subset = 'training',\n",
        "seed = 42,\n",
        "image_size = (img_height, img_width),\n",
        "batch_size = batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG59a9h_MiZW"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the validation dataset\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "data_dir,\n",
        "validation_split = 0.2,\n",
        "subset = 'validation',\n",
        "seed = 42,\n",
        "image_size = (img_height, img_width),\n",
        "batch_size = batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B33IL2Y5-nhb"
      },
      "source": [
        "<h4>Remove corrupted images from the test directory</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Upon looking at the images in the test directory for this dataset, it was observed that there was a number of corrupted images and so to fix that a function was built as seen below"
      ],
      "metadata": {
        "id": "e24VG5jK_Y2x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR4UO4iL-uh4"
      },
      "outputs": [],
      "source": [
        "# Function to remove corrupted images from test_dir\n",
        "\n",
        "def remove_corrupted_images(directory):\n",
        "\n",
        "  # Start iterating through the specified directory\n",
        "  for filename in os.listdir(directory):\n",
        "\n",
        "    # Defining the file path by joining the directory with the filename\n",
        "    file_path = os.path.join(directory, filename)\n",
        "\n",
        "    # trying to open and verify the image in file path\n",
        "    try:\n",
        "      # Try to open the image\n",
        "      img = Image.open(file_path)\n",
        "      img.verify()  # Additional verification\n",
        "\n",
        "    # If the verification failed remove the image at the file path\n",
        "    except (UnidentifiedImageError, OSError) as e:\n",
        "      # If UnidentifiedImageError or OSError occurs, the file is likely corrupted\n",
        "      print(f'Removing corrupted file: {file_path}')\n",
        "      os.remove(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK9POC5_-y1V"
      },
      "outputs": [],
      "source": [
        "# Calling the remove_corrupted_images function on the test directory to remove the corrupted images present\n",
        "\n",
        "remove_corrupted_images('/content/data/test/infected')\n",
        "remove_corrupted_images('/content/data/test/notinfected')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBpR4bbK0EfN"
      },
      "source": [
        "<h4>Preprocess the test dataset using ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC17zar2_RS_"
      },
      "outputs": [],
      "source": [
        "test_generator = ImageDataGenerator(rescale = 1.0 /255.0)    # Rescale the pixel values to range 0 - 1\n",
        "\n",
        "test_ds = test_generator.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    class_mode ='binary',\n",
        "    shuffle = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "598vwr_DO1PR"
      },
      "source": [
        "<h3>Visualize the Ovarian Ultrasound Images from DatasetA</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl8K2qriaTq0"
      },
      "source": [
        "<h4>Get the class names from the training dataset</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNsQoj2XOmPR"
      },
      "outputs": [],
      "source": [
        "# Get the class names\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "\n",
        "# Print the class names\n",
        "\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgugUyCubXZD"
      },
      "source": [
        "<h4>Display the count of the classes in the PCOS dataset</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPzZVefBOzut"
      },
      "outputs": [],
      "source": [
        "# Create a count plot of the classes in the PCOS dataset\n",
        "\n",
        "# Extract the labels from train_ds\n",
        "\n",
        "labels = []\n",
        "\n",
        "for idx, label in train_ds:\n",
        "    labels.extend(label.numpy().tolist())\n",
        "\n",
        "# Convert the labels to a NumPy array\n",
        "labels_array = np.array(labels)\n",
        "\n",
        "# Get the unique labels and their counts\n",
        "unique_labels, label_counts = np.unique(labels_array, return_counts = True)\n",
        "\n",
        "# Create a count plot using Matplotlib\n",
        "\n",
        "plt.figure(figsize = (8, 6))\n",
        "\n",
        "hist, bins, idx = plt.hist(labels_array, bins = np.arange(labels_array.min(), labels_array.max() +2) - 0.5,\n",
        "                         rwidth = 0.8, alpha = 0.75, color = 'pink', edgecolor = 'k')\n",
        "\n",
        "plt.xlabel(\"PCOS Dataset Classes\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(unique_labels, class_names)\n",
        "plt.title(\"Count Plot of the Classes in the PCOS Train set\", y = 1.05)\n",
        "\n",
        "\n",
        "# Add the total count for each class on top of each bar\n",
        "\n",
        "for i, count in enumerate(label_counts):\n",
        "\n",
        "    plt.text(unique_labels[i], count, str(count), ha = 'center', va = 'bottom')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qWUitiXb6na"
      },
      "source": [
        "<h4>Print 16 images with their labels from the training dataset<h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7PNu6AyPNM5"
      },
      "outputs": [],
      "source": [
        "# Create a figure for displaying images and set size to (10, 10)\n",
        "\n",
        "plt.figure(figsize = (10, 10))\n",
        "\n",
        "# Iterate over the first batch of images and labels in the training dataset\n",
        "for images, labels in train_ds.take(1):\n",
        "\n",
        "  # Loop through each image in the batch\n",
        "  for i in range(16):\n",
        "\n",
        "    # Create a subplot to display each image\n",
        "    ax = plt.subplot(4, 4, i + 1)\n",
        "\n",
        "    # Display the image\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "\n",
        "    # Set the title of the subplot to the corresponding class name\n",
        "    plt.title(class_names[labels[i]])\n",
        "\n",
        "    # Turn off axis labels\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "# Show the plot with images and class labels\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F8dt9_vPgSr"
      },
      "source": [
        "<h3>Data Augmentation</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Data augmentaion was applied to the train set to enhance learning\n",
        "+ The augmentaions applied can be seen below"
      ],
      "metadata": {
        "id": "WyKPSMQRAN1n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKhFzpMSPYUq"
      },
      "outputs": [],
      "source": [
        "# Data augmentation for the train dataset\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),  # Randomly flips the images horizontally.\n",
        "tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),       # Randomly rotates the images by up to 20%\n",
        "tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),           # Randomly zooms the images by up to 20%\n",
        "tf.keras.layers.experimental.preprocessing.Rescaling(1.0 / 255)       # Rescale the pixel values to range 0 - 1\n",
        "])\n",
        "\n",
        "# Apply data augmentation to the train dataset\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training = True), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Assigning class weights </h3>"
      ],
      "metadata": {
        "id": "BSzwnUgyArLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ As seen in the visualizations above, the dataset is not balanced and so to handle the issue of imbalancing, we assign class weights to the classes to ensure that they contribute equally in the training of the models"
      ],
      "metadata": {
        "id": "VqChTy6DGjS8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8FYjqzIgi-J"
      },
      "outputs": [],
      "source": [
        "# Calculate the class weights\n",
        "\n",
        "# Getting the class labels in the dataset\n",
        "labels = [label.numpy() for _, label in train_ds]\n",
        "\n",
        "# Joining the labels together and converting to a list\n",
        "labels = np.concatenate(labels).tolist()\n",
        "\n",
        "# Using the compute_class_weight method from the sklearn module to calculate the class weights\n",
        "class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(labels),\n",
        "                                        y = labels\n",
        "                                    )\n",
        "# Create a dictionary with the class names as keys and corresponding weights\n",
        "class_weights = dict(zip(np.unique(labels), class_weights))\n",
        "\n",
        "class_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORMdnqkKPtC-"
      },
      "source": [
        "<h3>Normalization</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ The data was normalized to promote faster convergence and improved generalization"
      ],
      "metadata": {
        "id": "rRCTPCC_JPio"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NDHNOl8Pea3"
      },
      "outputs": [],
      "source": [
        "# Normalization for val dataset\n",
        "\n",
        "val_normalization = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "\n",
        "# Apply normalization to the test dataset\n",
        "\n",
        "val_ds = val_ds.map(lambda x, y: (val_normalization(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Create a function to plot model history for training and accuracy </h3>"
      ],
      "metadata": {
        "id": "mUK5NZvbE4vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for plotting the history for training and validation accuracy and loss\n",
        "\n",
        "def plot_model_history(history):\n",
        "\n",
        "  # Plot the training accuracy against the validation accuracy\n",
        "\n",
        "  # Plot the training accuracy\n",
        "  plt.plot(history.history['accuracy'], label = 'Training Accuracy')\n",
        "\n",
        "  # Plot the validation accuracy\n",
        "  plt.plot(history.history['val_accuracy'], label = 'Validation accuracy')\n",
        "\n",
        "  plt.title('Training and Validation accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  # Display the plot\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # Plot the training loss against the validation loss\n",
        "\n",
        "  # Plot the training loss\n",
        "  plt.plot(history.history['loss'], label = 'Training loss')\n",
        "\n",
        "  #Plot the validation loss\n",
        "  plt.plot(history.history['val_loss'], label = 'validation loss')\n",
        "\n",
        "\n",
        "  plt.title('Training and Validation loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  # Display the plot\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "g-AS0K_LE5sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Create a function to generate classification report and confusion matrix </h3>"
      ],
      "metadata": {
        "id": "e5qtqYYrW1_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate classification report and confusion matrix\n",
        "\n",
        "def generate_classification_report_and_confusion_matrix(y_pred):\n",
        "\n",
        "  # Define the class labels\n",
        "  class_labels = ['infected', 'notinfected']\n",
        "\n",
        "  # Define the true or actual labels of the test dataset\n",
        "  y_true = test_ds.labels\n",
        "\n",
        "  # Printing the classification report\n",
        "  print(classification_report(y_true, y_pred, target_names = class_labels, digits = 4))\n",
        "\n",
        "  # Plotting the confusion matrix\n",
        "  cnn_cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "  plt.figure(figsize = (10, 8))\n",
        "\n",
        "  sns.heatmap(cnn_cm, annot = True, fmt = \"d\", cmap = \"Blues\", cbar = True, xticklabels = class_labels,\n",
        "              yticklabels = class_labels)\n",
        "\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.xlabel('Predicted Labels')\n",
        "  plt.ylabel('True Labels')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "pXtIt5quN77R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlvvmGXCP0if"
      },
      "source": [
        "<h2>CNN Model 1</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nio_9VfAoR_J"
      },
      "source": [
        "<h4>Define the CNN model's architecture</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtWxRzR2Pzxi"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "\n",
        "    # The input layer\n",
        "    layers.Conv2D(16, 3, padding = 'same', activation = 'relu', input_shape = (img_height,img_width, 3)),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # First hidden layer\n",
        "    layers.Conv2D(32, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Second hidden layer\n",
        "    layers.Conv2D(32, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Flattening layer\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # Dense layer\n",
        "    layers.Dense(64, activation = 'relu'),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "\n",
        "    # Output layer\n",
        "    layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "# Print the model's architecture summary\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0dCbbHIoaQ4"
      },
      "source": [
        "<h4>Train the CNN model</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihepiIwNQFdn"
      },
      "outputs": [],
      "source": [
        "# Compile the model using the defined optimizer, loss, and metrics\n",
        "\n",
        "model.compile(\n",
        "\n",
        "optimizer = Adam(1e-5),\n",
        "loss = 'BinaryCrossentropy',\n",
        "metrics = ['accuracy']\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TrLRbZaQUN8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Define the number of epochs for training\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "# Train the model using the train dataset and validate using the val dataset\n",
        "\n",
        "history = model.fit(\n",
        "train_ds,\n",
        "validation_data = val_ds,\n",
        "epochs = epochs,\n",
        "class_weight = class_weights\n",
        ")\n",
        "\n",
        "# Extract accuracy, validation accuracy, loss, and validation loss from the training history\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Define the range of epochs for plotting\n",
        "\n",
        "epochs_range = range(epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qC8oCkBxBIt"
      },
      "source": [
        "<h4>Plot the model history</h4>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_history(history)"
      ],
      "metadata": {
        "id": "Y9DD12XuQz_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEzfcH4c3KrC"
      },
      "source": [
        "<h4>Get the classification report and confusion matrix</h4>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Getting y_pred\n",
        "\n",
        "y_pred = model.predict(test_ds)\n",
        "\n",
        "y_pred = np.round(y_pred).flatten()"
      ],
      "metadata": {
        "id": "PP7BUmYHDmyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_classification_report_and_confusion_matrix(y_pred)"
      ],
      "metadata": {
        "id": "piIkI7JXSIFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjP-mglO_8O4"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "\n",
        "model.save('/content/drive/MyDrive/PCOS_detection_models/CNN_model1.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WxAcMX1ebtY"
      },
      "source": [
        "<h2>CNN Model 2</h2>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clears the background session before training a new model\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "NbuYEtT5SU-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Define the CNN model's architecture</h4>"
      ],
      "metadata": {
        "id": "bORWangITDdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "\n",
        "    # The input layer\n",
        "    layers.Conv2D(16, 3, padding = 'same', activation = 'relu', input_shape = (img_height,img_width, 3)),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # First hidden layer\n",
        "    layers.Conv2D(32, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Second hidden layer\n",
        "    layers.Conv2D(64, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Flattening layer\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # Dense layer\n",
        "    layers.Dense(128, activation = 'relu'),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "\n",
        "    # Output layer\n",
        "    layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "# Print the model's architecture summary\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ODKKtf3IS2B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model using the defined optimizer, loss, and metrics\n",
        "\n",
        "model.compile(\n",
        "\n",
        "optimizer = Adam(1e-6),\n",
        "loss = 'BinaryCrossentropy',\n",
        "metrics = ['accuracy']\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "0IkfT2afTTJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Define the number of epochs for training\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "# Train the model using the train dataset and validate using the val dataset\n",
        "\n",
        "history = model.fit(\n",
        "train_ds,\n",
        "validation_data = val_ds,\n",
        "epochs = epochs,\n",
        "class_weight = class_weights\n",
        ")\n",
        "\n",
        "# Extract accuracy, validation accuracy, loss, and validation loss from the training history\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Define the range of epochs for plotting\n",
        "\n",
        "epochs_range = range(epochs)"
      ],
      "metadata": {
        "id": "oOUrGunZThVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Plot the model history</h4>"
      ],
      "metadata": {
        "id": "6rdTVNnZZced"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_history(history)"
      ],
      "metadata": {
        "id": "rW7mcuJEXSe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Get the classification report and confusion matrix</h4>"
      ],
      "metadata": {
        "id": "ujub8nWIZiq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Getting y_pred\n",
        "\n",
        "y_pred = model.predict(test_ds)\n",
        "\n",
        "y_pred = np.round(y_pred).flatten()"
      ],
      "metadata": {
        "id": "-4qq5KIqD5ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_classification_report_and_confusion_matrix(y_pred)"
      ],
      "metadata": {
        "id": "08IROodkav2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "\n",
        "model.save('/content/drive/MyDrive/PCOS_detection_models/CNN_model2.keras')"
      ],
      "metadata": {
        "id": "LhhRbzUuU3Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>CNN Model 3</h2>"
      ],
      "metadata": {
        "id": "AITLj17LSQ0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clears the background session before training a new model\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "xxsj1L_HSErT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Define the CNN model's architecture</h4>"
      ],
      "metadata": {
        "id": "g6HkUw0_THzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "\n",
        "    # The input layer\n",
        "    layers.Conv2D(16, 3, padding = 'same', activation = 'relu', input_shape = (img_height,img_width, 3)),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Hidden layer\n",
        "    layers.Conv2D(32, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Flattening layer\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # Dense layer\n",
        "    layers.Dense(64, activation = 'relu'),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    # Output layer\n",
        "    layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "# Print the model's architecture summary\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RZ0fdVDlS4U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model using the defined optimizer, loss, and metrics\n",
        "\n",
        "model.compile(\n",
        "\n",
        "optimizer = Adam(1e-5),\n",
        "loss = 'BinaryCrossentropy',\n",
        "metrics = ['accuracy']\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "8h5RJlV5TVnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Define the number of epochs for training\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "# Train the model using the train dataset and validate using the val dataset\n",
        "\n",
        "history = model.fit(\n",
        "train_ds,\n",
        "validation_data = val_ds,\n",
        "epochs = epochs,\n",
        "class_weight = class_weights\n",
        ")\n",
        "\n",
        "# Extract accuracy, validation accuracy, loss, and validation loss from the training history\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Define the range of epochs for plotting\n",
        "\n",
        "epochs_range = range(epochs)"
      ],
      "metadata": {
        "id": "D71Z1z1bTjMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Plot the model history</h4>"
      ],
      "metadata": {
        "id": "P9V8dqC8Zm1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_history(history)"
      ],
      "metadata": {
        "id": "hipHzTIrXbvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Get the classification report and confusion matrix</h4>"
      ],
      "metadata": {
        "id": "Sj75MofUa_uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Getting y_pred\n",
        "\n",
        "y_pred = model.predict(test_ds)\n",
        "\n",
        "y_pred = np.round(y_pred).flatten()"
      ],
      "metadata": {
        "id": "1JR_JTOKD_Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_classification_report_and_confusion_matrix(y_pred)"
      ],
      "metadata": {
        "id": "GyhVH13Ma-Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "\n",
        "model.save('/content/drive/MyDrive/PCOS_detection_models/CNN_model3.keras')"
      ],
      "metadata": {
        "id": "oqaDZwTYVRG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>CNN Model 4</h2>"
      ],
      "metadata": {
        "id": "PxhmLrcpSXhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clears the background session before training a new model\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "VM5cyN6ISbXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Define the CNN model's architecture</h4>"
      ],
      "metadata": {
        "id": "efaDjgLUTKF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "\n",
        "    # The input layer\n",
        "    layers.Conv2D(32, 3, padding = 'same', activation = 'relu', input_shape = (img_height,img_width, 3)),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # First hidden layer\n",
        "    layers.Conv2D(32, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Second hidden layer\n",
        "    layers.Conv2D(64, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Third hidden layer\n",
        "    layers.Conv2D(64, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Flattening layer\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # Dense layer\n",
        "    layers.Dense(128, activation = 'relu'),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    # Output layer\n",
        "    layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "# Print the model's architecture summary\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "FGi6YtpYS6XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model using the defined optimizer, loss, and metrics\n",
        "\n",
        "model.compile(\n",
        "\n",
        "optimizer = Adam(1e-6),\n",
        "loss = 'BinaryCrossentropy',\n",
        "metrics = ['accuracy']\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "wpFh2MLuTYZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Define the number of epochs for training\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "# Train the model using the train dataset and validate using the val dataset\n",
        "\n",
        "history = model.fit(\n",
        "train_ds,\n",
        "validation_data = val_ds,\n",
        "epochs = epochs,\n",
        "class_weight = class_weights\n",
        ")\n",
        "\n",
        "# Extract accuracy, validation accuracy, loss, and validation loss from the training history\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Define the range of epochs for plotting\n",
        "\n",
        "epochs_range = range(epochs)"
      ],
      "metadata": {
        "id": "ijqbaXkeTlBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Plot the model history</h4>"
      ],
      "metadata": {
        "id": "V5lGUQNXZtWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_history(history)"
      ],
      "metadata": {
        "id": "nt4pI3g4aP85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Get the classification report and confusion matrix</h4>"
      ],
      "metadata": {
        "id": "mg9p2EUybTHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Getting y_pred\n",
        "\n",
        "y_pred = model.predict(test_ds)\n",
        "\n",
        "y_pred = np.round(y_pred).flatten()"
      ],
      "metadata": {
        "id": "GIqi0NGmEEXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_classification_report_and_confusion_matrix(y_pred)"
      ],
      "metadata": {
        "id": "V4Lzx9ETbThk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "\n",
        "model.save('/content/drive/MyDrive/PCOS_detection_models/CNN_model4.keras')"
      ],
      "metadata": {
        "id": "AyyckGQ0VU07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>CNN Model 5 - PCONet (Hosain et al, 2022))</h2>"
      ],
      "metadata": {
        "id": "OG60S4wMSZeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clears the background session before training a new model\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "OQr33vLBScNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Define the CNN model's architecture</h4>"
      ],
      "metadata": {
        "id": "Vs8LmrZkTMHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "\n",
        "    # The input layer\n",
        "    layers.Conv2D(32, 3, padding = 'same', activation = 'relu', input_shape = (img_height,img_width, 3)),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # First hidden layer\n",
        "    layers.Conv2D(32, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Second hidden layer\n",
        "    layers.Conv2D(64, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Second hidden layer\n",
        "    layers.Conv2D(64, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Third hidden layer\n",
        "    layers.Conv2D(128, 3, padding = 'same', activation = 'relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "\n",
        "    # Flattening layer\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # First Dense layer\n",
        "    layers.Dense(128, activation = 'relu'),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    # Second Dense layer\n",
        "    layers.Dense(256, activation = 'relu'),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    # Output layer\n",
        "    layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "# Print the model's architecture summary\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "hgWdjwF4S8Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model using the defined optimizer, loss, and metrics\n",
        "\n",
        "model.compile(\n",
        "\n",
        "optimizer = Adam(1e-5),\n",
        "loss = 'BinaryCrossentropy',\n",
        "metrics = ['accuracy']\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "2XUTR9-eTaRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Define the number of epochs for training\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "# Train the model using the train dataset and validate using the val dataset\n",
        "\n",
        "history = model.fit(\n",
        "train_ds,\n",
        "validation_data = val_ds,\n",
        "epochs = epochs,\n",
        "class_weight = class_weights\n",
        ")\n",
        "\n",
        "# Extract accuracy, validation accuracy, loss, and validation loss from the training history\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Define the range of epochs for plotting\n",
        "\n",
        "epochs_range = range(epochs)"
      ],
      "metadata": {
        "id": "YB7F0iR2Tm6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Plot the model history</h4>"
      ],
      "metadata": {
        "id": "k8Nje7-GZ0r0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_history(history)"
      ],
      "metadata": {
        "id": "ZjbA-RlDXqaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Get the classification report and confusion matrix</h4>"
      ],
      "metadata": {
        "id": "hXOHZ-OXbqzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Getting y_pred\n",
        "\n",
        "y_pred = model.predict(test_ds)\n",
        "\n",
        "y_pred = np.round(y_pred).flatten()"
      ],
      "metadata": {
        "id": "960uwUgVEJvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_classification_report_and_confusion_matrix(y_pred)"
      ],
      "metadata": {
        "id": "PhWbd_40brBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "\n",
        "model.save('/content/drive/MyDrive/PCOS_detection_models/CNN_model5.keras')"
      ],
      "metadata": {
        "id": "HUIgr8Q7VYOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Transfer Learning Models</h2>"
      ],
      "metadata": {
        "id": "gfoU1jxejhqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Define a class transfer_learning_models to build all the transfer learning models in this notebook<h4>"
      ],
      "metadata": {
        "id": "ClHSaXIpjnKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several methods in this class:\n",
        "+ Method to initializes the class transfer_learning_models\n",
        "+ Method to freeze the layers of the pre-trained model for initial training\n",
        "+ Method to unfreeze the layers and compile the pre-trained model to finetune the model\n",
        "+ Method to define the architecture of the pretrained model\n",
        "+ Method to fit the pretrained model on the PCOS dataset\n",
        "+ Method to fit the finetuned pretrained model on the PCOS dataset\n",
        "+ Method to plot the training and validation accuracy and loss for the pretrained model\n",
        "+ Method to print classification report and plot confusion matrix\n",
        "+ Method to save the models\n"
      ],
      "metadata": {
        "id": "TkabkTehjnZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a class transfer_learning_models\n",
        "\n",
        "class transfer_learning_models:\n",
        "\n",
        "    # Method to initializes the class transfer_learning_models\n",
        "    def __init__(self, base_model):\n",
        "\n",
        "        # Clears the background session before training a new model\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "        # Loads a pre-trained model\n",
        "        self.base_model = base_model(weights = 'imagenet', include_top = False,\n",
        "                                           input_shape = (224, 224, 3))\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.finetuned_history = None\n",
        "        self.model_name = None\n",
        "\n",
        "    # Method to freeze the layers of the pre-trained model\n",
        "    def freeze_pretrained_model_layers(self):\n",
        "\n",
        "        self.base_model.trainable = False\n",
        "\n",
        "    # Method to unfreeze the layers and compile the pre-trained model\n",
        "    def unfreeze_pretrained_model_layers(self, learning_rate):\n",
        "\n",
        "        self.base_model.trainable = True\n",
        "\n",
        "        self.model.summary()\n",
        "\n",
        "        self.model.compile(\n",
        "\n",
        "            optimizer = Adam(learning_rate),\n",
        "            loss = 'BinaryCrossentropy',\n",
        "            metrics = ['accuracy']\n",
        "        )\n",
        "\n",
        "    # Method to define the architecture of the pretrained model\n",
        "    def define_pretrained_model_architecture(self, dropout_value, learning_rate):\n",
        "\n",
        "        inputs = tf.keras.Input(shape = (224, 224, 3))\n",
        "        x = inputs\n",
        "        x = self.base_model(x, training = False)\n",
        "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "        x = tf.keras.layers.Dropout(dropout_value)(x)\n",
        "\n",
        "        outputs = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
        "\n",
        "        self.model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "        self.model.compile(\n",
        "\n",
        "            optimizer = Adam(learning_rate),\n",
        "            loss = 'BinaryCrossentropy',\n",
        "            metrics = ['accuracy']\n",
        "        )\n",
        "\n",
        "        return self.model.summary()\n",
        "\n",
        "\n",
        "    # Method to fit the pretrained model on the PCOS dataset\n",
        "    def fit_model(self, epochs):\n",
        "        self.history = self.model.fit(\n",
        "\n",
        "            train_ds,\n",
        "            epochs = epochs,\n",
        "            class_weight = class_weights,\n",
        "            validation_data = val_ds\n",
        "        )\n",
        "\n",
        "    # Method to fit the finetuned pretrained model on the PCOS dataset\n",
        "    def fit_fine_tuned_model(self, epochs):\n",
        "        self.fine_tuned_history = self.model.fit(\n",
        "            train_ds,\n",
        "            epochs = epochs,\n",
        "            class_weight = class_weights,\n",
        "            validation_data = val_ds,\n",
        "        )\n",
        "\n",
        "    # Method to plot the training and validation accuracy and loss for the pretrained model\n",
        "    def plot_accuracy_and_loss(self, model_name, acc_y_lower_lim, loss_y_upper_limit):\n",
        "\n",
        "        initial_epochs = self.history.epoch[-1]\n",
        "\n",
        "        acc = self.history.history['accuracy'] + self.fine_tuned_history.history['accuracy']\n",
        "        val_acc = self.history.history['val_accuracy'] + self.fine_tuned_history.history['val_accuracy']\n",
        "        loss = self.history.history['loss'] + self.fine_tuned_history.history['loss']\n",
        "        val_loss = self.history.history['val_loss'] + self.fine_tuned_history.history['val_loss']\n",
        "\n",
        "        # Calculate the y-axis tick positions for increments of 0.2\n",
        "        acc_y_ticks = np.arange(acc_y_lower_lim, 1.02, 0.02)\n",
        "        loss_y_ticks = np.arange(0, loss_y_upper_limit + 0.02, 0.02)\n",
        "\n",
        "\n",
        "\n",
        "        plt.figure(figsize = (8, 8))\n",
        "        plt.subplot(2, 1, 1)\n",
        "        plt.plot(acc, label = 'Training Accuracy')\n",
        "        plt.plot(val_acc, label = 'Validation Accuracy')\n",
        "\n",
        "        plt.ylim([acc_y_lower_lim, 1])\n",
        "        plt.plot([initial_epochs - 0.15, initial_epochs - 0.15],\n",
        "        plt.ylim(), label = 'Start Fine Tuning')\n",
        "        plt.legend(loc = 'lower right')\n",
        "        plt.title(f'Training and Validation Accuracy for {model_name}')\n",
        "        plt.subplot(2, 1, 2)\n",
        "        plt.plot(loss, label = 'Training Loss')\n",
        "        plt.plot(val_loss, label = 'Validation Loss')\n",
        "\n",
        "\n",
        "        plt.ylim([0, loss_y_upper_limit])\n",
        "        plt.plot([initial_epochs - 0.15,initial_epochs - 0.15],\n",
        "        plt.ylim(), label = 'Start Fine Tuning')\n",
        "        plt.legend(loc = 'upper right')\n",
        "        plt.title(f'Training and Validation Loss for {model_name}')\n",
        "        plt.xlabel('epoch')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    # Method to print classification report and plot confusion matrix\n",
        "    def classification_report_and_confusion_metrics(self, test_ds):\n",
        "      y_true = test_ds.labels\n",
        "\n",
        "      class_labels = ['infected', 'notinfected']\n",
        "\n",
        "      y_pred = self.model.predict(test_ds)\n",
        "\n",
        "      y_pred = np.round(y_pred).flatten()\n",
        "\n",
        "      print(classification_report(y_true, y_pred, target_names = class_labels, digits = 4))\n",
        "\n",
        "\n",
        "      cnn_cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "      plt.figure(figsize=(10, 8))\n",
        "\n",
        "      sns.heatmap(cnn_cm, annot = True, fmt = \"d\", cmap = \"Blues\", cbar = True, xticklabels = class_labels,\n",
        "                  yticklabels = class_labels)\n",
        "\n",
        "      plt.title('CNN Model Confusion Matrix')\n",
        "      plt.xlabel('Predicted Labels')\n",
        "      plt.ylabel('True Labels')\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "    # Method to save the models\n",
        "    def save_model(self, path_to_save_model):\n",
        "\n",
        "      self.model.save(path_to_save_model)"
      ],
      "metadata": {
        "id": "vL2VsUsyjlrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>InceptionV3 Model</h2>"
      ],
      "metadata": {
        "id": "RN0Ayv66kDy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates an instance of the class transfer_learning_models for InceptionV3\n",
        "\n",
        "InceptionV3 = transfer_learning_models(tf.keras.applications.InceptionV3)"
      ],
      "metadata": {
        "id": "I66fLfjzkAEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all the layers of the InceptionV3 model\n",
        "\n",
        "InceptionV3.freeze_pretrained_model_layers()"
      ],
      "metadata": {
        "id": "p6lWdZBlkALe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the architecture and print the summary for the InceptionV3 model\n",
        "\n",
        "InceptionV3.define_pretrained_model_architecture(0.2, 1e-4)"
      ],
      "metadata": {
        "id": "Gzj15Pv7kAQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Fit the InceptionV3 model on the PCOS dataset\n",
        "\n",
        "InceptionV3.fit_model(10)"
      ],
      "metadata": {
        "id": "tyzeW9_6kAV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetune the InceptionV3 model\n",
        "\n",
        "InceptionV3.unfreeze_pretrained_model_layers(1e-7)"
      ],
      "metadata": {
        "id": "_Nc-IU2jkAap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Fit the finetuned InceptionV3 model on the PCOS dataset\n",
        "\n",
        "InceptionV3.fit_fine_tuned_model(10)"
      ],
      "metadata": {
        "id": "bhKr_eL3kAld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation accuracy and loss\n",
        "\n",
        "InceptionV3.plot_accuracy_and_loss('InceptionV3', 0.50, 1.0)"
      ],
      "metadata": {
        "id": "02hV0Wy9ke3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Print the classification report and plot the confusion matrix\n",
        "\n",
        "InceptionV3.classification_report_and_confusion_metrics(test_ds)"
      ],
      "metadata": {
        "id": "pRJlzcRnkfXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the InceptionV3 model\n",
        "\n",
        "InceptionV3.save_model('/content/drive/MyDrive/PCOS_detection_models/InceptionV3.keras')"
      ],
      "metadata": {
        "id": "sbzDEZNBkmEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>MobileNetV2 Model</h2>"
      ],
      "metadata": {
        "id": "qXOwDisXkuvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates an instance of the class transfer_learning_models for MobileNetV2\n",
        "\n",
        "MobileNetV2 = transfer_learning_models(tf.keras.applications.MobileNetV2)"
      ],
      "metadata": {
        "id": "owqps4ZCkvmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all the layers of the MobileNetV2 model\n",
        "\n",
        "MobileNetV2.freeze_pretrained_model_layers()"
      ],
      "metadata": {
        "id": "nMvA-FExky3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the architecture and print the summary for the MobileNetV2  model\n",
        "\n",
        "MobileNetV2.define_pretrained_model_architecture(0.2, 1e-4)"
      ],
      "metadata": {
        "id": "6GJe_vWjky8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Fit the MobileNetV2 model on the PCOS dataset\n",
        "\n",
        "MobileNetV2.fit_model(10)"
      ],
      "metadata": {
        "id": "EwoR98-JkzA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetune the MobileNetV2 model\n",
        "\n",
        "MobileNetV2.unfreeze_pretrained_model_layers(1e-8)"
      ],
      "metadata": {
        "id": "iZaBy26fkzGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Fit the finetuned MobileNetV2 model on the PCOS dataset\n",
        "\n",
        "MobileNetV2.fit_fine_tuned_model(10)"
      ],
      "metadata": {
        "id": "W-vHictfkzMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation accuracy and loss\n",
        "\n",
        "MobileNetV2.plot_accuracy_and_loss('MobileNetV2', 0.50, 1.0)"
      ],
      "metadata": {
        "id": "0OW5C8GClCJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Print the classification report and plot the confusion matrix\n",
        "\n",
        "MobileNetV2.classification_report_and_confusion_metrics(test_ds)"
      ],
      "metadata": {
        "id": "XUH4-o-YlCtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the MobileNetV2 model\n",
        "\n",
        "MobileNetV2.save_model('/content/drive/MyDrive/PCOS_detection_models/MobileNetV2.keras')"
      ],
      "metadata": {
        "id": "4o1nXjc3lKOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>VGG16 Model</h2>"
      ],
      "metadata": {
        "id": "2JV_WPPmlQe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates an instance of the class transfer_learning_models for VGG16\n",
        "\n",
        "VGG16 = transfer_learning_models(tf.keras.applications.VGG16)"
      ],
      "metadata": {
        "id": "bn-KG3CBlWnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all the layers of the VGG16 model\n",
        "\n",
        "VGG16.freeze_pretrained_model_layers()"
      ],
      "metadata": {
        "id": "Ysdsohq2lZ86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the architecture and print the summary for the VGG16 model\n",
        "\n",
        "VGG16.define_pretrained_model_architecture(0.2, 1e-4)"
      ],
      "metadata": {
        "id": "83DHFXmtlaA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Fit the VGG16 model on the PCOS dataset\n",
        "\n",
        "VGG16.fit_model(10)"
      ],
      "metadata": {
        "id": "miO23GUblaFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetune the VGG16 model\n",
        "\n",
        "VGG16.unfreeze_pretrained_model_layers(1e-7)"
      ],
      "metadata": {
        "id": "yBLv7kMklaJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Fit the finetuned VGG16 model on the PCOS dataset\n",
        "\n",
        "VGG16.fit_fine_tuned_model(20)"
      ],
      "metadata": {
        "id": "Mkc6zPxclnVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation accuracy and loss\n",
        "\n",
        "VGG16.plot_accuracy_and_loss('VGG16', 0.50, 1.0)"
      ],
      "metadata": {
        "id": "mjj2F5d-ln7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Print the classification report and plot the confusion matrix\n",
        "\n",
        "VGG16.classification_report_and_confusion_metrics(test_ds)"
      ],
      "metadata": {
        "id": "gNDdmfZmloJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the VGG16 model\n",
        "\n",
        "VGG16.save_model('/content/drive/MyDrive/PCOS_detection_models/VGG16.keras')"
      ],
      "metadata": {
        "id": "AL3WpoVXlxa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>ResNet50 Model</h2>"
      ],
      "metadata": {
        "id": "Yq6wXk85l1SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates an instance of the class transfer_learning_models for ResNet50\n",
        "\n",
        "ResNet50 = transfer_learning_models(tf.keras.applications.ResNet50)"
      ],
      "metadata": {
        "id": "pXLASlgjl6Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all the layers of the ResNet50 model\n",
        "\n",
        "ResNet50.freeze_pretrained_model_layers()"
      ],
      "metadata": {
        "id": "bcG0qiXDl934"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the architecture and print the summary for the ResNet50  model\n",
        "\n",
        "ResNet50.define_pretrained_model_architecture(0.5, 1e-4)"
      ],
      "metadata": {
        "id": "kLnRHzY4l98A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Fit the ResNet50 model on the PCOS dataset\n",
        "\n",
        "ResNet50.fit_model(10)"
      ],
      "metadata": {
        "id": "eQkNRzcUl-DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetune the ResNet50 model\n",
        "\n",
        "ResNet50.unfreeze_pretrained_model_layers(1e-7)"
      ],
      "metadata": {
        "id": "VUBGNDqKl-Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Fit the finetuned ResNet50 model on the PCOS dataset\n",
        "\n",
        "ResNet50.fit_fine_tuned_model(50)"
      ],
      "metadata": {
        "id": "Mzz3yYtOmKQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation accuracy and loss\n",
        "\n",
        "ResNet50.plot_accuracy_and_loss('ResNet50', 0.50, 1.0)"
      ],
      "metadata": {
        "id": "T7V6SSD4mKUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Print the classification report and plot the confusion matrix\n",
        "\n",
        "ResNet50.classification_report_and_confusion_metrics(test_ds)"
      ],
      "metadata": {
        "id": "oTyFoys7mSsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the ResNet50 model\n",
        "\n",
        "ResNet50.save_model('/content/drive/MyDrive/PCOS_detection_models/ResNet 50.keras')"
      ],
      "metadata": {
        "id": "R_ZNmkh7mToG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLTb5MdCbMVSbS97Bj3APd"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}