{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOpFL5KlIsJhS+1MPlB0bQv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXptpBMiLoK4"
      },
      "source": [
        "<h1>A Machine Learning Hybrid Approach for PCOS Detection Using Ovarian Ultrasound Images</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeX8wtuoLbmo"
      },
      "source": [
        "<h3>Installing Kaggle<h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7WvCxcG8ChM"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Mount google drive to save the models and other variables in this notebook <h3>"
      ],
      "metadata": {
        "id": "57LeeUX_zNYI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S81vefKZM1ro"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Several operations were carried out below to download the datast from Kaggle </h3>"
      ],
      "metadata": {
        "id": "eWSyC9CKzRl9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2gQpTFCM7zT"
      },
      "outputs": [],
      "source": [
        "# Making a directory in the colab session that would hold my kaggle API\n",
        "\n",
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dRBFuwiN2kk"
      },
      "outputs": [],
      "source": [
        "# Copying the kaggle API credentials file to the newly created kaggle directory\n",
        "\n",
        "! cp /content/drive/MyDrive/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsx7Oa0DOBRd"
      },
      "outputs": [],
      "source": [
        "# Using the kaggle API to access my kaggle account\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RYcUOXGTUBp"
      },
      "outputs": [],
      "source": [
        "# Dowloading the pcos-detection-using-ultrasound-images dataset into the collab session using my kaggle account\n",
        "\n",
        "! kaggle datasets download -d anaghachoudhari/pcos-detection-using-ultrasound-images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EzIn3P_OAvq"
      },
      "outputs": [],
      "source": [
        "# Unzipping the pcos-detection-using-ultrasound-images dataset because Kaggle datasets are often in zipped format\n",
        "\n",
        "! unzip /content/pcos-detection-using-ultrasound-images.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Importing the libraries used in this notebook </h3>"
      ],
      "metadata": {
        "id": "MRDi4gcJznjx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ufJda8dKdaa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuGCfL5YMFwK"
      },
      "source": [
        "<h2>Data Preprocessing</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-ZwPXSpQ_qM"
      },
      "source": [
        "<h4>Define the data directory path</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY6QI3n6MMAk"
      },
      "outputs": [],
      "source": [
        "# Define the directory containing the training dataset\n",
        "\n",
        "data_dir = '/content/data/train'\n",
        "\n",
        "# Define the directory containing the testing dataset\n",
        "\n",
        "test_dir = '/content/data/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acCkx5TuRSZw"
      },
      "source": [
        "<h4>Define the batch size and image size</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjPovFgTMQh3"
      },
      "outputs": [],
      "source": [
        "# Define the batch size for training\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Define the dimensions for the images\n",
        "\n",
        "img_height = 224\n",
        "\n",
        "img_width = 224"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Load and preprocess the train dataset using ImageDataGenerator</h4>"
      ],
      "metadata": {
        "id": "fAOuAt5io3X0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Data augmentaion was applied to the train set to enhance learning\n",
        "+ The augmentaions applied can be seen below"
      ],
      "metadata": {
        "id": "UdvNy2Mlz3sw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TWbbNR4LLnX"
      },
      "outputs": [],
      "source": [
        "train_generator = ImageDataGenerator(\n",
        "\n",
        "    rescale = 1.0/255.0,     # Rescale the pixel values to range 0 - 1\n",
        "    horizontal_flip = True,  # Randomly flips the images horizontally\n",
        "    rotation_range = 0.2,    # Randomly rotates the images by up to 20%\n",
        "    zoom_range = 0.2         # Randomly zooms the images by up to 20%\n",
        "    )\n",
        "\n",
        "train_ds = train_generator.flow_from_directory(\n",
        "  data_dir,\n",
        "    target_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    class_mode=\"binary\",\n",
        "    shuffle = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Assigning class weights </h3>"
      ],
      "metadata": {
        "id": "WAxJ2Acp0gCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ The dataset is not balanced and so to handle the issue of imbalancing, we assign class weights to the classes to ensure that they contribute equally in the training of the models"
      ],
      "metadata": {
        "id": "yprOL4wf0gK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the class weights\n",
        "\n",
        "# Getting the class labels in the dataset\n",
        "labels = train_ds.classes\n",
        "\n",
        "# Using the compute_class_weight method from the sklearn module to calculate the class weights\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(labels),\n",
        "    y=labels\n",
        ")\n",
        "\n",
        "# Create a dictionary with the class names as keys and corresponding weights\n",
        "class_weights = dict(zip(np.unique(labels), class_weights))\n",
        "\n",
        "class_weights"
      ],
      "metadata": {
        "id": "FWyuk-iA-AOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiQCM0QVyoFQ"
      },
      "source": [
        "<h4>Remove corrupted images from the test directory</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Upon looking at the images in the test directory for this dataset, it was observed that there was a number of corrupted images and so to fix that a function was built as seen below"
      ],
      "metadata": {
        "id": "xFpBjYE41Kpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove corrupted images from test_dir\n",
        "\n",
        "def remove_corrupted_images(directory):\n",
        "\n",
        "  # Start iterating through the specified directory\n",
        "  for filename in os.listdir(directory):\n",
        "\n",
        "    # Defining the file path by joining the directory with the filename\n",
        "    file_path = os.path.join(directory, filename)\n",
        "\n",
        "    # trying to open and verify the image in file path\n",
        "    try:\n",
        "      # Try to open the image\n",
        "      img = Image.open(file_path)\n",
        "      img.verify()  # Additional verification\n",
        "\n",
        "    # If the verification failed remove the image at the file path\n",
        "    except (UnidentifiedImageError, OSError) as e:\n",
        "      # If UnidentifiedImageError or OSError occurs, the file is likely corrupted\n",
        "      print(f'Removing corrupted file: {file_path}')\n",
        "      os.remove(file_path)"
      ],
      "metadata": {
        "id": "We5JBSvS64nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the remove_corrupted_images function on the test directory to remove the corrupted images present\n",
        "\n",
        "remove_corrupted_images('/content/data/test/infected')\n",
        "remove_corrupted_images('/content/data/test/notinfected')"
      ],
      "metadata": {
        "id": "uv7iEmc6eaXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Preprocess the test dataset using ImageDataGenerator</h4>"
      ],
      "metadata": {
        "id": "bBpR4bbK0EfN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0iFGEmoxrc-"
      },
      "outputs": [],
      "source": [
        "test_generator = ImageDataGenerator(rescale = 1.0 /255.0)  # Rescale the pixel values to range 0 - 1\n",
        "\n",
        "test_ds = test_generator.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle = False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qDOjIwJGJ0A"
      },
      "source": [
        "<h2>Hybrid Model</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Spilt the dataset into training and testing sets</h4>"
      ],
      "metadata": {
        "id": "rb8Qj3erpken"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test = train_ds, train_ds.labels, test_ds, test_ds.labels"
      ],
      "metadata": {
        "id": "nN-neUI9ploS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Load the VGG16 base_model</h4>"
      ],
      "metadata": {
        "id": "s3Eyif25tiVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "VGG16_base_model = tf.keras.applications.VGG16(weights = 'imagenet', include_top = False, input_shape=(img_height, img_width, 3))"
      ],
      "metadata": {
        "id": "F0ujRX7Cp3xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Define the architecture of the feature_extractor</h4>"
      ],
      "metadata": {
        "id": "lMtxH_cG1wUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the layers of the VGG16_base_model\n",
        "\n",
        "VGG16_base_model.trainable = False\n",
        "\n",
        "# Define the architecture of the feature_extractor's\n",
        "\n",
        "inputs = tf.keras.Input(shape = (224, 224, 3))\n",
        "\n",
        "x = inputs\n",
        "x = VGG16_base_model(x, training = False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
        "\n",
        "feature_extractor = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Remove the last layer\n",
        "\n",
        "feature_extractor = tf.keras.Model(inputs = feature_extractor.input, outputs = feature_extractor.layers[-2].output)\n",
        "\n",
        "# Print the feature_extractor's summary\n",
        "\n",
        "feature_extractor.summary()"
      ],
      "metadata": {
        "id": "t1tkemqjvUG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Extract the features from the train set\n",
        "\n",
        "train_features = feature_extractor.predict(x_train)"
      ],
      "metadata": {
        "id": "nBHDR5zvqLUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_for_stacking = train_features"
      ],
      "metadata": {
        "id": "X286DxcNwEX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Extract the features from the test set\n",
        "\n",
        "test_features = feature_extractor.predict(x_test)"
      ],
      "metadata": {
        "id": "fsqQmzu5xUoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Stacking Model 2</h3>"
      ],
      "metadata": {
        "id": "MEE6zlHPl-ll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Create a function to define the architecture of the stacking model</h3>"
      ],
      "metadata": {
        "id": "qjr_G3DU105s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h97Ea4sjSkDg"
      },
      "outputs": [],
      "source": [
        "# Function to define the architecture of the stacking model\n",
        "\n",
        "def get_stacking():\n",
        "\n",
        "        level0 = []\n",
        "        level0.append(('Random_Forest', RandomForestClassifier(class_weight = class_weights)))\n",
        "        level0.append(('XGBoost', XGBClassifier(scale_pos_weight = class_weights[1])))\n",
        "\n",
        "        level1 = LogisticRegression(class_weight = class_weights)\n",
        "\n",
        "        model = StackingClassifier(estimators = level0, final_estimator = level1, cv = 5)\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the get_stacking function\n",
        "\n",
        "stacker = get_stacking()"
      ],
      "metadata": {
        "id": "YX7v2Lj3JqPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Fitting the x_for_stacking and y_train on the stacked model\n",
        "\n",
        "stacker.fit(x_for_stacking, y_train)"
      ],
      "metadata": {
        "id": "76WpvAmlJ-Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Create a function to generate classification report and confusion matrix </h3>"
      ],
      "metadata": {
        "id": "nWJBSbKF2Ic1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate classification report and confusion matrix\n",
        "\n",
        "def generate_classification_report_and_confusion_matrix(y_pred):\n",
        "\n",
        "  # Define the class labels\n",
        "  class_labels = ['infected', 'notinfected']\n",
        "\n",
        "  # Define the true or actual labels of the test dataset\n",
        "  y_true = test_ds.labels\n",
        "\n",
        "  # Printing the classification report\n",
        "  print(classification_report(y_true, y_pred, target_names = class_labels, digits = 4))\n",
        "\n",
        "  # Plotting the confusion matrix\n",
        "  cnn_cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "  plt.figure(figsize = (10, 8))\n",
        "\n",
        "  sns.heatmap(cnn_cm, annot = True, fmt = \"d\", cmap = \"Blues\", cbar = True, xticklabels = class_labels,\n",
        "              yticklabels = class_labels)\n",
        "\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.xlabel('Predicted Labels')\n",
        "  plt.ylabel('True Labels')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "AeGOcR3B2KAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Get the classification report and confusion matrix</h4>"
      ],
      "metadata": {
        "id": "hw5bkAGw6qPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Getting y_pred\n",
        "\n",
        "y_pred = stacker.predict(test_features)"
      ],
      "metadata": {
        "id": "MO9MjRdh6YGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_classification_report_and_confusion_matrix(y_pred)"
      ],
      "metadata": {
        "id": "9r54Fbvj6qgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate classification report and confusion matrix for training\n",
        "\n",
        "def generate_classification_report_and_confusion_matrix_training(train_y_pred):\n",
        "\n",
        "  # Define the class labels\n",
        "  class_labels = ['infected', 'notinfected']\n",
        "\n",
        "  # Define the true or actual labels of the test dataset\n",
        "  y_true = test_ds.labels\n",
        "\n",
        "  # Printing the classification report\n",
        "  print(classification_report(y_train, train_y_pred, target_names = class_labels, digits = 4))\n",
        "\n",
        "  # Plotting the confusion matrix\n",
        "  cnn_cm = confusion_matrix(y_train, train_y_pred,)\n",
        "\n",
        "  plt.figure(figsize = (10, 8))\n",
        "\n",
        "  sns.heatmap(cnn_cm, annot = True, fmt = \"d\", cmap = \"Blues\", cbar = True, xticklabels = class_labels,\n",
        "              yticklabels = class_labels)\n",
        "\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.xlabel('Predicted Labels')\n",
        "  plt.ylabel('True Labels')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "k0sTRlMICga1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Check for overfitting</h4>"
      ],
      "metadata": {
        "id": "DtQQzclCD_3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting train_y_pred\n",
        "\n",
        "train_y_pred = stacker.predict(train_features)"
      ],
      "metadata": {
        "id": "g4TvRTzn_jme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for overfitting by generating the classification report and confusion matrix for training\n",
        "\n",
        "generate_classification_report_and_confusion_matrix_training(train_y_pred)"
      ],
      "metadata": {
        "id": "KhKPoQgUDXIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Testing Hybrid Model on a Low Quality Dataset A</h2>"
      ],
      "metadata": {
        "id": "W2MLw3dcZT3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Preprocess the test dataset using ImageDataGenerator</h4>"
      ],
      "metadata": {
        "id": "ET4w8gkD7J0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to blur images to create a low quality dataset\n",
        "\n",
        "def blur_images(image):\n",
        "\n",
        "    # Apply Gaussian blur to the image\n",
        "\n",
        "    blurred_image = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "\n",
        "    return blurred_image\n",
        "\n",
        "test_generator = ImageDataGenerator(\n",
        "\n",
        "    rescale=1.0 / 255.0,   # Rescale the pixel values to range 0 - 1\n",
        "\n",
        "    preprocessing_function = blur_images\n",
        ")\n",
        "\n",
        "test_ds = test_generator.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle = False\n",
        ")"
      ],
      "metadata": {
        "id": "xIgdCRnZZWNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Spilt the dataset into training and testing sets</h4>"
      ],
      "metadata": {
        "id": "3QbmIKPi7eLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test = train_ds, train_ds.labels, test_ds, test_ds.labels"
      ],
      "metadata": {
        "id": "ydmFYlo8ZwVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Extract the features from the train set\n",
        "\n",
        "train_features = feature_extractor.predict(x_train)"
      ],
      "metadata": {
        "id": "IX0OQirUZxHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_for_stacking = train_features"
      ],
      "metadata": {
        "id": "5hAFOOq9ZxNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Extract the features from the test set\n",
        "\n",
        "test_features = feature_extractor.predict(x_test)"
      ],
      "metadata": {
        "id": "HkRiefv2ZxRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the get_stacking function\n",
        "\n",
        "stacker = get_stacking()"
      ],
      "metadata": {
        "id": "GdTQ4bP3ZxUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Fitting the x_for_stacking and y_train on the stacked model\n",
        "\n",
        "stacker.fit(x_for_stacking, y_train)"
      ],
      "metadata": {
        "id": "7LNvO5w1ZxXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Get the classification report and confusion matrix</h4>"
      ],
      "metadata": {
        "id": "6N-9-5-g7olc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Getting y_pred\n",
        "\n",
        "y_pred = stacker.predict(test_features)"
      ],
      "metadata": {
        "id": "DI_KhJ3yALxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_classification_report_and_confusion_matrix(y_pred)"
      ],
      "metadata": {
        "id": "IHd1MNH47o1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4>Check for overfitting</h4>"
      ],
      "metadata": {
        "id": "ert8zVsqDxt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting train_y_pred\n",
        "\n",
        "train_y_pred = stacker.predict(train_features)"
      ],
      "metadata": {
        "id": "PsGmqt9SAUwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for overfitting by generating the classification report and confusion matrix for training\n",
        "\n",
        "generate_classification_report_and_confusion_matrix_training(train_y_pred)"
      ],
      "metadata": {
        "id": "MV9jawtvDu7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "rYPEtJvA_itA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}